{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/brandonbryant/Desktop/fivethirtyeight-trump-twitter/data/realdonaldtrump_poll_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = df['text'].values\n",
    "goodTweets = []\n",
    "for tweet in tweets:\n",
    "    if 'RT' in tweet:\n",
    "        continue\n",
    "    if '@' in tweet:\n",
    "        continue\n",
    "    goodTweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for tweet in tweets:\n",
    "    text = text + \" \" + tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53773"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text = text.replace('.', \"\")\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', tagger=False, entity=False)\n",
    "doc = nlp(text.lower())\n",
    "\n",
    "tokens = []\n",
    "for token in doc:\n",
    "    if not 'http' in str(token):\n",
    "        tokens.append(str(token))\n",
    "        \n",
    "\n",
    "n_tokens = len(tokens)\n",
    "tokens_to_int = dict((c, i) for i, c in enumerate(tokens))\n",
    "n_vocab = len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens:  10535\n",
      "Total Vocab:  1914\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Tokens: \", n_tokens)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  10524\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 11\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_tokens - seq_length, 1):\n",
    "    seq_in = tokens[i:i + seq_length]\n",
    "    seq_out = tokens[i + seq_length]\n",
    "    gen = ([tokens_to_int[token]] for token in seq_in)\n",
    "    dataX.append([next(gen)[0],next(gen)[0],next(gen)[0],next(gen)[0],next(gen)[0],next(gen)[0],next(gen)[0], next(gen)[0],next(gen)[0],next(gen)[0],next(gen)[0]])\n",
    "    dataY.append(tokens_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 7.0619\n",
      "Epoch 00001: loss improved from inf to 7.06318, saving model to weights-improvement-01-7.0632.hdf5\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 7.0632\n",
      "Epoch 2/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.4166\n",
      "Epoch 00002: loss improved from 7.06318 to 6.41479, saving model to weights-improvement-02-6.4148.hdf5\n",
      "10524/10524 [==============================] - 30s 3ms/step - loss: 6.4148\n",
      "Epoch 3/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.3484\n",
      "Epoch 00003: loss improved from 6.41479 to 6.35135, saving model to weights-improvement-03-6.3513.hdf5\n",
      "10524/10524 [==============================] - 30s 3ms/step - loss: 6.3513\n",
      "Epoch 4/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2985\n",
      "Epoch 00004: loss improved from 6.35135 to 6.29867, saving model to weights-improvement-04-6.2987.hdf5\n",
      "10524/10524 [==============================] - 32s 3ms/step - loss: 6.2987\n",
      "Epoch 5/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2740\n",
      "Epoch 00005: loss improved from 6.29867 to 6.27511, saving model to weights-improvement-05-6.2751.hdf5\n",
      "10524/10524 [==============================] - 33s 3ms/step - loss: 6.2751\n",
      "Epoch 6/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2639\n",
      "Epoch 00006: loss improved from 6.27511 to 6.26308, saving model to weights-improvement-06-6.2631.hdf5\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 6.2631\n",
      "Epoch 7/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2698\n",
      "Epoch 00007: loss did not improve\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 6.2682\n",
      "Epoch 8/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2738\n",
      "Epoch 00008: loss did not improve\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 6.2724\n",
      "Epoch 9/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2705\n",
      "Epoch 00009: loss did not improve\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 6.2699\n",
      "Epoch 10/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2673\n",
      "Epoch 00010: loss did not improve\n",
      "10524/10524 [==============================] - 32s 3ms/step - loss: 6.2674\n",
      "Epoch 11/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2688\n",
      "Epoch 00011: loss did not improve\n",
      "10524/10524 [==============================] - 32s 3ms/step - loss: 6.2687\n",
      "Epoch 12/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2664\n",
      "Epoch 00012: loss did not improve\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 6.2656\n",
      "Epoch 13/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2611\n",
      "Epoch 00013: loss improved from 6.26308 to 6.26186, saving model to weights-improvement-13-6.2619.hdf5\n",
      "10524/10524 [==============================] - 30s 3ms/step - loss: 6.2619\n",
      "Epoch 14/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2707\n",
      "Epoch 00014: loss did not improve\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 6.2717\n",
      "Epoch 15/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2672\n",
      "Epoch 00015: loss did not improve\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 6.2664\n",
      "Epoch 16/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2671\n",
      "Epoch 00016: loss did not improve\n",
      "10524/10524 [==============================] - 30s 3ms/step - loss: 6.2657\n",
      "Epoch 17/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2737\n",
      "Epoch 00017: loss did not improve\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 6.2750\n",
      "Epoch 18/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2638\n",
      "Epoch 00018: loss did not improve\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 6.2659\n",
      "Epoch 19/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2646\n",
      "Epoch 00019: loss did not improve\n",
      "10524/10524 [==============================] - 30s 3ms/step - loss: 6.2658\n",
      "Epoch 20/20\n",
      "10496/10524 [============================>.] - ETA: 0s - loss: 6.2623\n",
      "Epoch 00020: loss did not improve\n",
      "10524/10524 [==============================] - 31s 3ms/step - loss: 6.2632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1145e4400>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-13-6.2619.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_token = dict((i, c) for i, c in enumerate(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generateTrumptweets = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,1):\n",
    "    # pick a random seed\n",
    "    start = np.random.randint(0, len(dataX)-1)\n",
    "    pattern = dataX[start]\n",
    "    tweet = \"\"\n",
    "    tweet = tweet + ''.join([int_to_token[value] for value in pattern])\n",
    "    # generate characters\n",
    "    for i in range(40):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(n_vocab)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_token[index]\n",
    "        seq_in = [int_to_token[value] for value in pattern]\n",
    "        #print(result)\n",
    "        tweet = tweet + result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "        #print(pattern[1:len(pattern)])\n",
    "    generateTrumptweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'keepingitrealhttp://tco/asrjp5acmz\"i\\'mleadingbybigmargins\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateTrumptweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
